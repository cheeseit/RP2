\subsection{Validation vs No Validation}
\label{dis:val}
%TODO validation no validation 32 nodes or not.
Figure \ref{fig:val_vs_noval} shows that approximation to divide the search time by the total number of edges instead of the actually visited edges by Angel et al.\cite{angel2012graph} is a rough approximation. The scales used for the validation experiments were small and different behavior might be observed for higher scales. To further investigate approximation, experiments need to be done on a platform with a newer version of MPI installed, because then validation can be left on.

\subsection{DAS}
There are two kinds of tipping points in these graphs. The first can be seen in figure \ref{fig:nodes_no_val}, in which the TEPS as a function of nodes is shown. For example, a tipping point can be seen at 8 nodes with scale 12. After this tipping point, adding more nodes leads to a sharp decrease in performance. Thus this tipping point, shows the optimal number of nodes for a certain scale. In the case of the DAS-4 results in figure \ref{fig:das_no_val}, this is for 8 nodes for scale 12. Figure \ref{fig:scale_no_val} shows the other tipping point, which can be seen at scale 21 for 32 nodes. This tipping point shows the point where paralellization has reached its peak. Adding more data(i.e., using a graph of larger scale) to the application will not improve the performance anymore. 


%In the results in figure \ref{fig:das_no_infini} and  \ref{fig:das_no_val} following can be seen. There is a tipping point at which the number of TEPS decreases. This is unlike what can be seen in the experiments without InfiniBand (and it also not observed on Amazon). 
%There is a big difference between the experiments with and without InfiniBand. The performance results on DAS-4 when using Infiniband show a much more drastic dip in the performance after the tipping point, whereas the experiments without InfiniBand performance show almost no degradation. This is most likely because of a the network being saturated which is mentioned in the paper by Angel et al.\cite{angel2012graph}. This paper suggest that at a certain point the network starts to play a role.
%TODO need to think about this some more.

\subsection{OpenNebula}
There were some problems with getting the program to work on OpenNebula. The cluster on which the latest version of OpenNebula was installed was shutdown in preparation of the new DAS-5. For this reason the older version 3.8 needed to be used. The OpenNebula marketplace does not store any images for version 3.8 anymore, which made the setup time longer. Also public images available on the OpenNebula VU cluster did not work out of the box.
One thing to note is that when you are running MPI it is best not to have a firewall between the hosts. The MPI nodes need to be able to open ports to each other for the messages.  

\subsection{Amazon}
Experiments on the Amazon EC2 were only done once and on one region. The results did not show much variance, the standard deviation is about one percent of the mean value.

The experiments were run on Friday and over the weekend. We expect that, during the day or peak hours the performance might can change, due to higher demands on Amazon EC2 (assuming these instances, r3.large and c3.large, see demand variability over time). More experiments should have been done to confirm that the cloud is this stable. The performance might also change per region. 

Using Amazon EC2 does have some limitations. With a new account you can only make a limited amount of machines. To raise this limit you need to present a business case. This might not be a problem in most cases, but for huge experiments might not be possible. Getting the machines ready is also a huge task when working with more instances. Starting up 32 machines on amazon took longer 15 minutes and the VMs also need to the latest code after creation. When doing experiments with a lot amount of nodes it is better to create a special image where everything is ready. After all the machines are ready, all the machines need to be added to the \texttt{known\_hosts} file(see Appendix \ref{app:amazon preparation} for the Amazon setup scripts.). If this is not done the MPI program will get stuck, because it needs to accept a key. Adding all machines to the \texttt{known\_hosts} might also take a long time, depending on how this is done.


\subsection{Reference implementation}
\input{sections/problems.tex}

\subsection{Model}
The initial intuition for the model is that there are two factors which contribute to calculating the TEPS, namely: the computational time and the communication time. The computation time is the  taken to traverse all the edges in the graph. The communication time is the time it takes to send all the messages. Trying to make a model with these two parameter resulted in straight lines. The computation is a linear function of the number of nodes and the communication is a function of the scale and the number of nodes, because everything is parallel and the number of and the messages which are sent are non-blocking the calculations need to be done for one node.
The paper by Angel proposes that there is another factor which is contention on the network. What the paper suggest is that at some point the network is getting saturated. This is due to the shear amount of nodes that need to be transported over the network. This is also what can be seen in the results. As the nodes stay constant and the scale goes up there is a tipping point as mentioned in the results. After this tipping point the amount of TEPS keeps reducing. This can be explained, by looking at contention. If the scale goes up but the number of nodes stay the same the chance of contention keeps increasing. As the contention increases the network will reach is minimum value which is the maximum one link can handle. This means that it will return to a state between two nodes instead of having a parallel system of $n$ nodes.
This problem would be less prominent in the Amazon EC2 environment. In this environment congestion will occur much less likely because the network is much larger and there are no direct connections between the machines which might get clogged.
%TODO think about this some more.
With this new insight in mind there are multiple ways to test if this is true. One easy way to increase the buffer size. By increasing the buffer size the amount of messages that will not decrease but the timings of these messages will differ and there will not be a continues stream coming from one node only bursts.However, this test is left as future work. 



 