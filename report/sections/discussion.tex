We discus our results, the model, other findings and problems encountered in the following section.

\subsection{Validation vs No Validation}
\label{dis:val}
%TODO validation no validation 32 nodes or not.
The estimation to use the total number of edges compared to the visited actual number of edges is rough estimation. To find out how rough this estimation is experiments need to be done on a system with a newer version of MPI installed. Although this is a rough estimation the behavior is the same in both the experiments with and without validation.
The scales that were used to make the comparison are very small. In the Amazon EC2 these are the scales that show a different behavior, but for the DAS-4 with and without InifiBand it did not. 

\subsection{DAS}
In the results in figure \ref{fig:das_no_infini} and  \ref{fig:das_no_val} following can be seen. There is a tipping point at which the number of TEPS decreases. This is unlike what can be seen in the experiments without InfiniBand and on AWS. There are two kinds of tipping points in these graphs. The first can be seen in  figure \ref{fig:nodes_no_val}, in which the TEPS as a function of nodes is shown. In this figure a tipping point can be seen at 8 nodes with scale 12. All larger number of nodes decreases the amount of performance that can be achieved. The tipping point, in this case shows the optimal number of nodes for a certain scale. In the case of the DAS-4 results in figure \ref{fig:das_no_val} it is for 8 nodes and scale 12.

The other tipping that can be found in it's counter part. Figure \ref{fig:scale_no_val} shows a tipping point at scale 21 for 32 nodes. This tipping point shows the point where paralellization has reached it peak. Adding more data to the application will not improve the performance anymore. 

There is a big difference between the experiments with and without InfiniBand. The performance for the DAS experiments show a much more drastic dip in the performance after the tipping point, whereas the experiments without infiniband it stays constant. This is most likely because of a the network being saturated which is mentioned in the paper by Angel et al.\cite{angel2012graph}. This paper suggest that at a certain point the network starts to play a role.  After the maximum achievable performance has been reached the performance goes down because the InfiniBand links will become saturated. They explain it as following: the final part of the program the chance that multiple processes need to write to the same process becomes higher, thus saturating the network. When this happens the multiple InfiniBand links turn into fewer ones because some of them are clogged. This has not been investigated further, but is something to look into. 
%TODO need to think about this some more.

\subsection{OpenNebula}

There were some problems with getting the program to work on OpenNebula. The cluster on which the latest version of OpenNebula was installed was shutdown in preparation of the new DAS-5. For this reason the older version 3.8 needed to be used. The OpenNebula marketplace does not store any images for version 3.8 anymore, which made the setup time longer. Also public images available on the OpenNebula VU cluster did not work out of the box.
One thing to note is that when you are running MPI it is best not to have a firewall between the hosts. The MPI nodes need to be able to open ports to each other for the messages.  

\subsection{Amazon}
Experiments on the Amazon EC2 were only done once and on one region. The results did not show much variance, the standard deviation is about one percent of the mean value.

The experiments were run on friday and over the weekend. During the day or peak hours the performance might be change. More experiments should have been done to confirm that the cloud is this stable. The performance might also change per region. 


Using Amazon EC2 does have some limitations. With a new account you can only make a limited amount of machines. To raise this limit you need to present a business case. This might not be a problem in most cases, but for huge experiments might not be possible. Getting the machines ready is also a huge task when working with more instances. Starting up 32 machines on amazon took longer 15 minutes and they also had to provisioned afterwards. When doing experiments with a huge amount of nodes it is better to create a special image where everything is ready. After all the machines are ready, all the machines need to be added to the \texttt{known\_hosts} file. If this is not done the MPI program will get stuck, because it needs to accept a key. Adding all machines to the \texttt{known\_hosts} might also take a long time, depending on how this is done.


\subsection{Reference implementation}
\input{sections/problems.tex}

\subsection{Model}
When we first thought of the model, we thought there were two factors which contribute to calculating the TEPS, namely: the computational time and the communication time. The computation time is a constant is the amount of nodes that one node can compute. The communication time is the time it takes to send all the messages. Trying to make a model with these two parameter resulted in straight lines. The computation is a linear function of the number of nodes and the communication is a function of the scale and the number of nodes, because everything is parallel and the number of and the messages which are sent are non-blocking the calculations need to be done for one node. 
The paper by Angel proposes that there is another factor which is contention on the network. What the paper suggest is that at some point the network is getting saturated. This is due to the shear amount of nodes that need to be transported over the network. This is also what can be seen in the results. As the nodes stay constant and the scale goes up there is a tipping point as mentioned in the results. After this tipping point the amount of TEPS keeps reducing. This can be explained, by looking at contention. If the scale goes up but the number of nodes stay the same the chance of contention keeps increasing. As the contention increases the network will reach is minimum value which is the maximum one link can handle. This means that it will return to a state between two nodes instead of having a parallel system of n nodes.
%TODO think about this some more.
With this new insight in mind there are multiple ways to test if this is true. One easy way to increase the buffer size. By increasing the buffer size the amount of messages that will not decrease but the timings of these messages will differ and there will not be a continues stream coming from one node only bursts.



 