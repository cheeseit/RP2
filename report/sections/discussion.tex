In this section the results, the model, other findings and problems encountered will be discussed.

\subsection{DAS}
In the results seen in graphs \ref{fig:das_no_infini}, \ref{fig:das_no_val} following can be seen. In this figure you can see that there is a tipping point at which the number of TEPS decreases. This is unlike what can be seen in the experiments without InfiniBand and on AWS. These show a tipping point a small dip and then a steady decline as the scale increases. But the DAS experiments show a much more drastic dip in the performance after the tipping point. This is most likely because of a the network being saturated which is mentioned in the paper by angel. This paper suggest that at a certain point point the . There are 3 factors which contribute to the amount of edges that can be traversed, namely: the computation time, the communication time and the network. The maximum achievable TEPS the one that takes the maximum amount of time divided by the. The more nodes you have the more spread out the information will and the more often the information needs to be gotten from the another machine. At the start of the experiment all nodes will have a lot of nodes to process and the connection will be fine, but as the number of edges which needs to be the same nodes need to access to the same node which will clog the network.


\subsection{OpenNebula}
The results of OpenNebula the order of magnitude is much lower than what can be seen in Amazon and the DAS-4 without InifiBand even thought it is running on the same hardware as.

There were some problems with using OpenNebula. The cluster on which the latest version of OpenNebula was installed was shutdown in preparation of the new DAS-5. For this reason the older version 3.8 needed to be used. The OpenNebula marketplace does not have any images for version 3.8 anymore, which made the setup time a bit longer. Also public images available on the OpenNebula VU cluster seemed to be working out of the box. Also one thing to note is that when you are running MPI it is best not to have a firewall between the hosts. 

\subsection{Amazon}
The results seen in figure \ref{fig:c3_amazon} and \ref{fig:r3_amazon} look consistent with the thought that the number of TEPS is a function of the time of computation and the time to pass messages. 

Creating on demand machine on Amazon is easy, but there are some limitation

\subsection{Validation vs No Validation}
%TODO validation no validation 32 nodes or not.
AS shown in the figure \ref{fig:val_vs_noval} the ratio between TEPS as a function of nodes is a constant factor that does not exceed the 100 \% increase. This means that the estimation that is done using the no validation experiments are comparable to expected value which is given by the experiments with validation. The estimation to use the total number of edges compared to the visited actual number of edges is a good approximation and will give an answer in the right order of magnitude. 

\subsection{Reference implementation}
\input{sections/problems.tex}

\subsection{Model}
When we first thought of the model, we thought there were two factors which contribute to calculating the TEPS, namely: the computational time and the communication time. The computation time is a constant is the amount of nodes that one node can compute. The communication time is the time it takes to send all the messages. Trying to make a model with these two parameter resulted in straight lines. The computation is a linear function of the number of nodes and the communication is a function of the scale and the number of nodes, because everything is parallel and the number of and the messages which are sent are non-blocking the calculations need to be done for one node. 
The paper by Angel proposes that there is another factor which is contention on the network. What the paper suggest is that at some point the network is getting saturated. This is due to the shear amount of nodes that need to be transported over the network. This is also what can be seen in the results. As the nodes stay constant and the scale goes up there is a tipping point as mentioned in the results. After this tipping point the amount of TEPS keeps reducing. This can be explained, by looking at contention. If the scale goes up but the number of nodes stay the same the chance of contention keeps increasing. As the contention increases the network will reach is minimum value which is the maximum one link can handle. This means that it will return to a state between two nodes instead of having a parallel system of n nodes.
%TODO think about this some more.
With this new insight in mind there are multiple ways to test if this is true. One easy way to increase the buffer size. By increasing the buffer size the amount of messages that will not decrease but the timings of these messages will differ and there will not be a continues stream coming from one node only bursts.



 