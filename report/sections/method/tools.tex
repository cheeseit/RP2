\subsubsection{Reference Implementations}

%TODO detailed explanation about the algorithm how the division is done between the node. Layers. Need to be edited alot Combined to different pieces
The Graph 500 reference code\cite{graph500-code} used is version 2.1.4. This version has four different MPI implementations which all perform the BFS in the same way. The differences between the implementations are the data structures used to store the graph and the way the graph is distributed. The names of the applications are: \texttt{graph500\_mpi\_simple}, \texttt{graph500\_mpi\_one\_sided}, \texttt{graph500\_mpi\_replicated} and \texttt{graph500\_mpi\_replicated\_csc}.
\\ 
The \texttt{graph500\_mpi\_one\_sided} is not be considered, because the one sided implementation expects high performance remote memory access to work properly. This is a technique which can not be relied on in the public cloud, because you have no control over the environment. The \texttt{graph500\_mpi\_replicated} and \texttt{graph500\_mpi\_replicated\_csc} store the complete graph in each of the nodes. This will take a lot of RAM per node. We would like to use as little hardware as possible on the public cloud, so we avoided these applications.

Thus, we are using \texttt{graph500\_mpi\_simple}. Other reasons to choose this application are: it is the most simple to understand, it has been thoroughly studied and it requires the least amount of RAM on each of the nodes. 

A detailed description of the \texttt{graph500\_mpi\_simple} application can be found in \cite{suzumura2011performance}.

The implementation uses 2 queues for the BFS. The first queue is used to store all nodes that should still be visited in this iteration. The second queue is used to store all the nodes that should be visited in the next iteration. When the first queue is empty the roles will be swapped of the queues and the next iteration will start. This is done until there are no more nodes that should be visited. On each level the vertices are evenly distributed between all participating processes. The graph is stored by using Compressed Row Storage\cite{crs} to minimize the amount of data that needs to be stored in the RAM.

\begin{lstlisting}[label={code:pseudo-simple},caption={Pseudo code taken from paper \cite{suzumura2011performance}}]
for all vertex v do 
   |  pred[v] ← -1; 
   |  visited[v] ← 0; 
   CQ  ← Empty; 
   NQ ← Empty; 
   CQ[root] ← 1; 
   
fork;
   this ← GetMyRank(); 
   loop 
 |  while CQ != Empty do 
 |  |  for each received vertex v and its predecessor u do 
 |  |  |  if visited[v] = 0 then 
 |  |  |  |  visited[v] ← 1; 
 |  |  |  |  pred[v] ← u; 
 |  |  |  |  Enqueue(NQ, v); 
 |  |  u ← Dequeue(CQ); 
 |  |  for each vertex v adjacent to u do 
 |  |  |  r ← GetOwner(v); 
 |  |  |  if r = this then 
 |  |  |  |  if visited[v] = 0 then 
 |  |  |  |  |  visited[v] ← 1; 
 |  |  |  |  |  pred[v] ← u; 
 |  |  |  |  |  Enqueue(NQ, v); 
 |  |  |  else 
 |  |  |  |  send (v, u) to r; 
 |  if new queue of all the processes is empty then 
 |  |  break; 
 |  
swap(CQ, NQ); 
  join;
\end{lstlisting}
In the pseudo code CQ is the current queue and NQ is the queue for the next level.
 

\subsection*{Initial modeling}
Graph traversal is a traditional memory-intensive application. When running on a distributed system, the inter-node communication can also become a bottleneck, depending on the speed of the nodes and the communication links. 
The paper by Suzumura \cite{suzumura2011performance} proposes an estimate of the amount of communication in the \texttt{graph500\_mpi\_simple} . The formula is:
\begin{equation}
\label{eq:communication_size}
C(n, M) = A * B * C * D \text{ (bytes)}.
\end{equation}
Where $A = M*2$ , $B = (n-1)/n$, $C=2$, $D=8$, $M$ = total number of edges, and $n$ = the number of MPI processes. The outcome of the equation is the communication size in bytes. \\
Knowing the amount of data is useful to calculate the amount of messages that need be sent and could also be used to calculate the network load.

\subsubsection{Message Passing Interface and OpenMP}
\label{mpiopenmp}
The Graph 500 reference code is created in C and uses MPI and OpenMP to parallelize the program.

``Message Passing Interface (MPI) is a standardized and portable message-passing system designed by a group of researchers from academia and industry to function on a wide variety of parallel computers.''\cite{mpi}

``OpenMP is an implementation of multithreading, a method of parallelizing whereby a master thread (a series of instructions executed consecutively) forks a specified number of slave threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors.''\cite{openmp}.


\subsubsection{Intel MPI Benchmark}
\label{tools-imb}
Because Graph 500 is a communication intensive benchmark, we need to understand the inter-node communication performance. For this we use the Intel MPI benchmark(IMB).  
The IMB is a free benchmark used to determine how well MPI performs on a certain platform. The benchmark consists of a few different tests, each measuring a different aspect of MPI. Of these tests ``PingPong'' is the one important to us. ``PingPong''\cite{img-userguide} is used to measure the start up and throughput of a single message sent between two processes.

For our purposes, IMB's ``PingPong'' is compiled using the OpenMPI compiler; details of the compilation can be found in Appendix \ref{app:imb}.

