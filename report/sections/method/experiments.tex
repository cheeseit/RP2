This section explains which experiments have been done and which parameters are used.

\subsubsection{Communication}
\label{med:comm}
%Needs to be revised and more  precise.
The MPI Graph 500 benchmark is a communication heavy application. The time it takes to send messages might have an impact on the performance. To evaluate how much time is spent on sending messages, the total amount of messages send needs to be approximated. The model shown in equation \ref{eq:communication_size} approximates inter-node communication size. This can be used to calculate the total number of messages that needs to be sent. To confirm that the model fits application,the messages have logged each time a MPI message is send by a node. By doing this total number messages send can be observed. 

There are 3 types of messages sent in this program. The first type is sent when the message buffer is full. The buffer has a fixed size (2 kB) in each of the experiments but it can be changed to get better performance. 

The second type of message is used to flush the buffer. If all nodes have been visited by the program and the buffer has some vertices for other nodes visit, the node will flush the buffer.

The last message is used to report that the program is done. The node sends an empty message to other nodes meaning that the processing send the message is done visiting vertices.

Because the communication is asynchronous all nodes can send messages at the same time. The time it takes for one node to send all it's messages is the communication time of the whole program. We assume that on average all nodes need to send the same amount of messages. With this assumption in mind the total number of messages needs to be divided by the number of nodes to get the number of messages per node. This can then be multiplied by the time it takes to send a message and you will have the theoretical communication time.  

The IMB benchmark will be done for each of the environments to measure the time it takes to send messages (see section \ref{tools-imb}).


\subsubsection{Measurements}
The MPI program takes two parameter: scale and edgefactor. The measurements are run for the scale 9,12,15,21,24,27 and 30 on 2,4,6,16 and 32 nodes. Each node of the MPI program will only contain one process. The benchmark will be run one time for each of the experiments. It is run only one time because the benchmark already does 64 different searches to get statics of the performance. The platforms on which the measurements will be done are: DAS-4, OpenNebula and Amazon Webservices. The specifications of these platforms can be found in section \ref{hardware}.

The memory consumption is important determine which scales can be run on which machines and how many of the machines will be needed. The following table was created table \ref{tab:calculation memory consumption}. The table is created by using the following equation which predicts the memory consumption:
\begin{equation}
M(Scale) = (2^{Scale} *(2*edgefactor + 1)) * 8
\end{equation}
\begin{table} [!h]
	\begin{center}
		\begin{tabular}{|l|l|}
			\hline
			Scale & Predicted total memory use (GB) \\ \hline
			9 &  0.000135168 \\ \hline
			12 & 0.001081344 \\ \hline
			15 & 0.008650752 \\ \hline
			18 & 0.069206016 \\ \hline
			21 & 0.553648128 \\ \hline
			24 & 4.429185024 \\ \hline
			27 & 35.433480192 \\ \hline
			30 & 283.467841536 \\ \hline
		\end{tabular}
	\end{center}
	
	\caption{The predicted total amount of memory used by the program. To find out how much memory is used per node divide the total by the number of nodes that will be used.}
	\label{tab:calculation memory consumption}
\end{table}


\subsubsection{OpenMP}
The \texttt{simple\_mpi\_simple} implements OpenMP for some of the loops in the program. To find out what the performance effect of OpenMP is, one node was tested with different amount of CPUs assigned the program. By default, OpenMP will decide how many nodes will be used for the problem at hand by checking the amount of available cores. By forcing OpenMP to use a specific amount of CPUs, the influence of the number of CPUs on the performance can be measured. For the OpenMP experiments a small change needed to be made in the code to turn off the dynamic assignment, such that the number of CPUs to be used can be specified. The experiments were done on one node of the LU cluster. The experiments were only done for a limited number of scales, because only one node is used. Validation was still turned on for these experiments. We expect that using more CPUs for the program will improve the performance.

\subsubsection{MPI}
The MPI measurements have been conducted to find out the effect of the number of nodes and scale on the performance\footnote{Technically experiments could be done with 64 nodes, but some of the nodes were out of service and even if all nodes were available there is always someone else using one of other nodes which made it impossible to run an experiment with 64 nodes.}.
The experiments are done on the DAS-4, OpenNebula and the Amazon EC2. The DAS-4 and OpenNebula experiments serve as a baseline for the Amazon EC2 results. DAS-4 will serve as the comparison of the cloud results to a super computer. The OpenNebula will serve as comparison for another cloud platform. The following experiments sets were done:
\begin{enumerate}
	\item With validation
	\item Without validation
	\item Without Infiniband
	\item On the OpenNebula
	\item On Amazon EC2 c3.large and Amazon EC2 r3.large 
\end{enumerate}

The first two sets of experiments are, the \texttt{graph500\_mpi\_simple} with and without validation. These experiments are done to decide if turning of the validation has an impact on the results and how much large this impact is. We expect the results to be similar in behavior, but the experiments without validation will give a higher performance, because the execution time is divided by the total number of edges instead of the number of visited edges.

The third set of experiments, running the bechmark without InfiniBand, will provide a better comparison between the DAS-4 and Amazon EC2. The cloud does not have InfiniBand, turning will bring the results closer together with this the locality of the environment can be measured. An Amazon data center is larger than the DAS-4 and the machines will most likely be further apart, this means that the results will most likely be comparable, but the Amazon EC2 would perform worse.

The OpenNebula experiments are done in the same environment as the DAS-4 experiments, but use different hardware and a virtualization layer on top. The expected results from the OpenNebula experiments is that they will be comparable to the results of the Amazon EC2 and the DAS-4.

The expectations of the Amazon EC2 set of experiments, is that the results is will be comparable to the DAS-4 without Infiniband, but have lower performance.

\subsubsection*{DAS-4}
To run programs on the DAS-4, \texttt{prun} needs to be used. This program reserves nodes and places all the required files on the node for the \texttt{mpirun} command. For \texttt{prun} to run the specified program, it needs to run a script to find out all environment variables and what flags should be used for \texttt{mpirun}. One adjustment needed to be made to the default script to get \texttt{prun} working for \texttt{graph500\_mpi\_simple}. The file can be found in Appendix[DAS command].

\subsubsection*{OpenNebula}
Running experiments on OpenNebula was done by using \texttt{mpirun}. The experiments done with 2, 4, 8 and 16 nodes each of the VMs got a 24 GB of memory. This amount has been chosen because this is the same amount as the nodes on the VU cluster. For the experiment with 32 VMs, the nodes have only 10 GB. 10GB was chosen because the eight physical nodes could not handle 32 nodes with the same specification. With this setup, the experiments could still be run. The VMs used for the OpenNebula experiments have CentOS release 5.11 (Final) installed. The version of MPI was 1.4. On the VMs no InfiniBand has been installed.

\subsubsection*{Amazon EC2}
For the experiments on Amazon EC2 a new image was created similar to the image used on OpenNebula. To create this image a publicly available image was used with 5.x version of CentOS. There is one difference between the experiments done on OpenNebula and the Amazon EC2. The \texttt{mpirun} command is started from one of the participating nodes instead of running it from a node which is not participating. Running MPI in such a manner might have an influence on the performance.
