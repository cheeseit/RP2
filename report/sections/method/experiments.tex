This section explains which experiments have been done and which parameters are used.

\subsubsection{Measurements}
The MPI program takes two parameter: scale and edgefactor. The edgefactor is The measurements are run for the scale 9,12,15,21,24,27 and 30 on 2,4,6,16 and 32 nodes. Each node of the MPI program will only contain one process. The benchmark will be run one time for each of the experiments. It is run only one time because the benchmark already does 64 different. The platforms on which the measurements will be done are: DAS-4, OpenNebula and Amazon Webservices. The specifications of these platforms can be found in section \ref{hardware}.

The memory consumption is important determine which scales can be run on which machines and how many of the machines will be needed. The following table was created table \ref{tab:calculation memory consumption}. The table is created by using the following equation which predicts the memory consumption:
\begin{equation}
M(Scale) = (2^{Scale} *(2*edgefactor + 1)) * 8
\end{equation}
\begin{table} [!h]
	\begin{center}
		\begin{tabular}{|l|l|}
			\hline
			Scale & Predicted total memory use (GB) \\ \hline
			9 &  0.000135168 \\ \hline
			12 & 0.001081344 \\ \hline
			15 & 0.008650752 \\ \hline
			18 & 0.069206016 \\ \hline
			21 & 0.553648128 \\ \hline
			24 & 4.429185024 \\ \hline
			27 & 35.433480192 \\ \hline
			30 & 283.467841536 \\ \hline
		\end{tabular}
	\end{center}
	
	\caption{The predicted total amount of memory used by the program. To find out how much memory is used per node divide the total by the number of nodes that will be used.}
	\label{tab:calculation memory consumption}
\end{table}

\subsubsection{OpenMP}
The \texttt{simple\_mpi\_simple} implements OpenMP for some of the loops in the program. To find out what the performance effect of OpenMP is, one node was tested with different amount of CPUs assigned the program. By default, OpenMP will decide how many nodes will be used for the problem at hand by checking the amount of available cores. By forcing OpenMP to use a specific amount of CPUs, the influence of the number of CPUs on the performance can be measured. For the OpenMP experiments a small change needed to be made in the code to turn off the dynamic assignment, such that the number of CPUs to be used can be specified. The experiments were done on one node of the LU cluster. The experiments were only done for a limited number of scales, because only one node is used. Validation was still turned on for these experiments. We think that using more nodes for the program will improve the performance.

\subsubsection{MPI}

As seen in section \ref{hw:das4} and \ref{hw:opennebula} there can be experiments done with up to 16 nodes on the LU cluster and up to 32 nodes on the VU. \footnote{Technically experiments could be done with 64 nodes on the VU cluster, but some of the nodes were out of service and there is always someone else using one of the nodes which made it impossible to run an experiment with 64 nodes.}
The experiments are done on the DAS-4 and OpenNebula as a reference for the experiments on the commercial cloud. 
Note that the simple algorithm only allows for node amounts which are a power of 2. Different experiments were done for the MPI. The following types experiments were done:
\begin{itemize}
	\item With validation
	\item Without validation
	\item Without Infiniband
	\item On the OpenNebula
	\item On Amazon EC2 c3.large and Amazon EC2 r3.large 
\end{itemize}


\subsubsection*{DAS-4}
To run program on the DAS-4 \texttt{prun} needs to be used. This program reserves nodes and places all the required files on the node for the \texttt{mpirun} command. For \texttt{prun} to run the specified program it needs to run a script to find out all environment variables and what flags should be used for \texttt{mpirun}. There was one small adjustment made to the default script to get \texttt{prun} working for \texttt{graph500\_mpi\_simple}. The file can be found in Appendix[DAS command].

\subsubsection*{OpenNebula}
Running experiments on OpenNebula was done by using \texttt{mpirun} with the fitting parameters. The experiments done with 2 4,8 and 16 nodes each of the VMs got a 24 GB of memory. This amount has been chosen because this is the same amount as the nodes on the VU cluster. For the experiment with 32 VMs the nodes have only 10 GB. 10GB was chosen because the 8 nodes could not handle 32 nodes with the same specification as used for experiments before. With this setup the experiments could still be run. The VMs used for the OpenNebula experiments have CentOS release 5.11 (Final) installed. The version of MPI was 1.4. On the VMs no InfiniBand has been installed. The commands used for the installation can be found in Appendix [REf appendix installation]

\subsubsection*{Amazon EC2}
For the experiments on Amazon EC2 a new HVM image was created similar to the VM used on OpenNebula. The base of this image is  a public image(ami-15011a61) which also has the 5.x version of CentOS. On this image keys were placed, MPI installed and the git repository downloaded. There is one difference between the experiments done on OpenNebula and the Amazon EC2. The \texttt{mpirun} command is started from one of the nodes instead of from a node which does not participate in the calculation. This is done setting up an \texttt{SSH} connection to the server and then running the command remotely, for the command see Appendix [REF to used commands]. Running MPI in this manner might have some influence on the performance of that one node. The installation of the software was done in exactly the same way as for OpenNebula see Appendix[Ref appendix installation].


\subsubsection{Communication}
\label{med:comm}
%Needs to be revised and more  precise.
To confirm that the model used for the communication is sound, messages have logged each time a MPI message is send. By doing this the amount of messages can be found.

There are 3 types of messages sent in this program. The first type is sent when the message buffer is full. The buffer has a fixed size (2 kB) in each of the experiments but it can be changed to get better performance. 

The second type of message is ot flush. If all nodes have been visited by the program and the buffer still has some nodes that need to be visited it will flush the buffer with remaining vertices.

The last message is used to report that the program is done. The node sends an empty message to other nodes meaning that the node is done visiting vertices.

Also the IMB benchmark will be done for each of the environments to measure the time it takes to send messages \ref{tools:imb}.


    

