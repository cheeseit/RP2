This section explains which experiments have been done and which parameters are used.

\subsubsection{Communication}
\label{med:comm}
%Needs to be revised and more  precise.
The MPI Graph 500 benchmark is a communication heavy application. The time it takes to send messages might have an impact on the performance. To evaluate how much time is spent on sending messages, the total amount of messages send needs to be approximated. The model shown in equation \ref{eq:communication_size} approximates inter-node communication size. This can be used to calculate the total number of messages that needs to be sent. To confirm that the model fits application, we have logged each time a MPI message is send by a node. By doing this, the total number messages sent can be observed. 

There are 3 types of messages sent in this program. The first type is sent when the message buffer is full. The buffer has a fixed size (2 kB) in each of the experiments but it can be changed to get better performance. 

The second type of message is used to flush the buffer. If all nodes have been visited by the program and the buffer has some vertices for other nodes visit, the node will flush the buffer.

The last message is used to report that the program is done. The node sends an empty message to other nodes meaning that sender is done visiting vertices.

Because the communication is asynchronous, all nodes can send messages at the same time. The time it takes for one node to send all it's messages is the communication time of the whole program. We assume that on average all nodes need to send the same amount of messages. This assumption is based on the observation that the queues generate a reasonable load balancing for the application. With this assumption in mind the total number of messages needs to be divided by the number of nodes to get the number of messages per node. This can then be multiplied by the time it takes to send a message to obtain the theoretical communication time.  

The IMB benchmark will be performed for each of the environments to measure the time it takes to send messages (see section \ref{tools-imb}).


\subsubsection{Measurements}
The MPI program takes two parameters: scale and edgefactor. The measurements are run for the scale 9,12,15,21,24,27 and 30 on 2,4,6,16 and 32 nodes. Each node of the MPI program will only contain one process. The benchmark will be run one time for each of the experiments. It is run only one time because the benchmark already does 64 different searches to get statics of the performance. The platforms on which the measurements will be done are: DAS-4, OpenNebula and Amazon Webservices. The specifications of these platforms can be found in section \ref{hardware}.

The memory consumption is important determine which scales can be run on which machines and how many machines are needed. Equation \ref{eq:memory_usage} determines the memory footprint of each scale, and it is used to calculate the memory footprint of different scales, seen in table \ref{tab:calculation memory consumption} which predicts the memory consumption:
\begin{equation}
\label{eq:memory_usage}
M(Scale) = (2^{Scale} *(2*edgefactor + 1)) * 8
\end{equation}
\begin{table} [!h]
	\begin{center}
		\begin{tabular}{|l|l|}
			\hline
			Scale & Predicted total memory consumption (GB) \\ \hline
			9 &  0.000135168 \\ \hline
			12 & 0.001081344 \\ \hline
			15 & 0.008650752 \\ \hline
			18 & 0.069206016 \\ \hline
			21 & 0.553648128 \\ \hline
			24 & 4.429185024 \\ \hline
			27 & 35.433480192 \\ \hline
			30 & 283.467841536 \\ \hline
		\end{tabular}
	\end{center}
	
	\caption{The predicted total amount of memory used by the program. To find out how much memory is used per node divide the total by the number of nodes that will be used.}
	\label{tab:calculation memory consumption}
\end{table}


\subsubsection{OpenMP}
The \texttt{simple\_mpi\_simple} uses OpenMP for some of the loops in the program. To find out what the performance effect of OpenMP is, we have tested the benchmark on a single node, by measuring the performance when fixing different numbers of CPUs to use. By default, OpenMP will decide how many nodes will be used for the problem at hand by checking the amount of available cores. By forcing OpenMP to use a specific amount of CPUs, the influence of the number of CPUs on the performance can be measured. For the OpenMP experiments a small change needed to be made in the code to turn off the dynamic assignment, such that the number of CPUs to be used can be specified. The experiments were done on one node of the LU cluster. The experiments were only done for a limited number of scales, because only one node is used. Validation was still turned on for these experiments. We expect that using more CPUs for the program will improve the performance.

\subsubsection{MPI}
\label{med:mpi}
The MPI measurements have been conducted to find out the effect of the number of nodes and scale on the performance\footnote{Technically experiments could be done with 64 nodes, but some of the nodes were out of service and even if all nodes were available there is always someone else using one of other nodes which made it impossible to run an experiment with 64 nodes.}.
The experiments are done on the DAS-4, OpenNebula and the Amazon EC2. The DAS-4 and OpenNebula experiments serve as a baseline for the Amazon EC2 results. DAS-4 will serve as the comparison of the cloud results to a super computer. The OpenNebula will serve as comparison for another cloud platform. The following experiments sets were done:
\begin{enumerate}
	\item With validation
	\item Without validation
	\item Without Infiniband
	\item On the OpenNebula
	\item On Amazon EC2 c3.large and Amazon EC2 r3.large 
\end{enumerate}

The first two sets of experiments are, the \texttt{graph500\_mpi\_simple} with and without validation. These experiments are done to decide if turning of the validation has an impact on the results and how much large this impact is. We expect the results to be similar in behavior, but the experiments without validation will give a higher performance, because the execution time is divided by the total number of edges instead of the number of visited edges.

The third set of experiments, running the bechmark without InfiniBand, will provide a better comparison between the DAS-4 and Amazon EC2. The cloud does not have InfiniBand, turning will bring the results closer together with this the locality of the environment can be measured. An Amazon data center is larger than the DAS-4 and the machines will most likely be further apart, this means that the results will most likely be comparable, but the Amazon EC2 would perform worse.

The OpenNebula experiments are done in the same environment as the DAS-4 experiments, but use different hardware and a virtualization layer on top. The expected results from the OpenNebula experiments is that they will be comparable to the results of the Amazon EC2 and the DAS-4.

The expectations of the Amazon EC2 set of experiments, is that the results is will be comparable to the DAS-4 without Infiniband, but have lower performance.

\subsubsection*{Removing validation}
To run the experiments for scales larger than 15, validation needed to be removed. For these scales the program gets stuck while validating. During the validation also the performance of the search is calculated. To be able to run the program without validation\footnote{The graph does not need validation, because this is the reference implementation} and still measure the performance, the following has been done. Instead of dividing BFS search time by the number of traversed edges during the search it will be divided by the total number of edges as proposed in \cite{angel2012graph}. Calculating the performance in this way can have a impact and needs to be investigated. Therefore the first set experiments were run. The code with the changes to remove validation can be found here \cite{rp2-github}


\subsubsection*{DAS-4}
To run programs on the DAS-4, \texttt{prun} needs to be used. This program reserves nodes and places all the required files on the node for the \texttt{mpirun} command. For \texttt{prun} to run the specified program, it needs to run a script to find out all environment variables and what flags should be used for \texttt{mpirun}. One adjustment needed to be made to the default script to get \texttt{prun} working for \texttt{graph500\_mpi\_simple}. The file can be found in Appendix \ref{app:das-environment}.

\subsubsection*{OpenNebula}
Running experiments on OpenNebula was done by using \texttt{mpirun}. The experiments done with 2, 4, 8 and 16 nodes had 24 GB of memory available. This amount has been chosen because this is the same amount of memory as the nodes on the VU cluster. For the experiment with 32 VMs, the nodes have only 10 GB. 10GB was chosen because the eight physical nodes could not handle 32 nodes with 24 GB of RAM. With this setup, the experiments could still be run. The VMs used for the OpenNebula experiments have CentOS release 5.11 (Final) installed. The version of MPI was 1.4. On the VMs no InfiniBand has been installed.

\subsubsection*{Amazon EC2}
For the experiments on Amazon EC2 a new image was created similar to the image used on OpenNebula. To create this image a publicly available image was used with 5.x version of CentOS. There is one difference between the experiments done on OpenNebula and the Amazon EC2. The \texttt{mpirun} command is started from one of the participating nodes instead of running it from a node which is not participating. Running MPI in such a manner might have an influence on the performance.