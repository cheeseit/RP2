Our results are promising so far, but more research needs to be done in validating and refining the model.

To validate the proposed model, more experiments still need to be run. Larger amount of nodes and higher scales should be tested. Behaviors can differ for larger scales. It has only been shown that the behavior is different for smaller, because the tipping point has been identified empirically. The same behavior could also occur for larger scales and more nodes.

The \texttt{graph500\_mpi\_simple} could run multiple processes on one node. The program does not make use of multiple processes during the BFS. This means that if there is enough memory on the computer it would be possible to run multiple processes per node. This has already been shown in the paper by Angel et al.\cite{angel2012graph}, but it is something that we have not looked at yet.

Other implementations could also be used to run on the cloud. The \texttt{graph500\_mpi\_simple} is a not an optimized implementation. The performance can be improved by having a more optimized implementation on each node. The implementation should be hardware agnostic, if it is to run on cloud. The implementation by \cite{ueno2012highly} which do a two dimensional version of the BFS algorithm might be a good first step.
 
Furthermore, other types of cloud instances and public cloud services should be tested. Only Amazon EC2 has been benchmarked in this project and only two of its instances. Although these instances seemed the most relevant for this project, larger instances should be tested to find out if the model still holds. Also the, using the m3.large might provide a good comparison to the other results. The Google cloud might also be interesting option to run the application on.

DAS-4 with InfiniBand showed a different behavior than all the other experiments we have performed. This behavior is unrelated to the Graph 500 on the cloud, but it is interesting to investigate further.