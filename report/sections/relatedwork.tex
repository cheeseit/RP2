There is a lot of related work on the Graph500 benchmark. Most of these papers focus on the implementation of better BFS kernels and are not applicable for our study.

In a paper, by Chaktranont et al.\cite{chakthranont2014exploring}, the difference is shown between \texttt{graph500\_mpi\_simple} on a virtual private cluster and on a physical cluster. The paper shows that the  virtualization overhead head is about 5\% on the HPC Cloud they created. This is a cloud solution specifically created for super computers. Comparing this with other cloud solutions might give some insights.

Toyotaro Suzumura et al. \cite{suzumura2011performance} investigated the reference Graph 500 implementation. The paper gives detailed explanation of three of the four implementations and provides a performance evaluation of these implementations. They managed to reach 8 GTEPS for a scale 34 problem and provide some insight about how to optimize the algorithms. The implementation used in this project is described and their evaluation and communication model are used.

A technical paper by Angel et al.\cite{angel2012graph} runs the Graph500 simple implementation on UMBC High Performance Computing Facility. In this paper the simple implementation runs on their cluster up to scale 32 and 64 nodes. They share a way of removing the validation from the program. The experiments are done by running multiple instances of the program on the same node for up to 64 nodes and explain the implication of running the program in such a way. The hardware used is similar to the DAS-4. They also propose a way to turn off validation, which is also used in our project.

To summarize, there has been a lot of work done on performing and optimizing the Graph 500 benchmark, but to our knowledge no one has attempted to run the Graph 500 benchmark on a public cloud yet.