There is a lot of related work on the Graph 500 benchmark. Most papers focus on the implementation of better BFS kernels and are not applicable to our study.

In a paper, by Chaktranont et al.\cite{chakthranont2014exploring}, the difference is shown between \texttt{graph500\_mpi\_simple} on a virtual private cluster and on a physical cluster. The paper shows that the  virtualization overhead head is about 5\% on the HPC Cloud they created. This is a cloud solution specifically created for super computers. Comparing their results with other cloud solutions might give some insights.

Toyotaro Suzumura et al. \cite{suzumura2011performance} investigated the Graph 500 reference implementations. The paper gives a detailed explanation of three of the four implementations and provides a performance evaluation of these implementations. Suzumaru et al. managed to reach eight GTEPS for a scale 34 problem. The paper also provides insights into optimizing the reference implementations. The implementation used in this project is described in their paper. Their insights about the implementations and communication model have been used for our research.

A technical paper by Angel et al.\cite{angel2012graph} runs the Graph 500 simple implementation on UMBC High Performance Computing Facility. In this paper the simple implementation runs on their cluster up to scale 32 and 64 nodes. They share a way of removing the validation from the program. The experiments are done by running multiple instances of the program on the same node for up to 64 nodes and explain the implication of running the program in such a way. The hardware used by Angel et al. is similar to the DAS-4. They also propose a method of turning off validation, which is used in our project.

To summarize, there has been a lot of work done on performing and optimizing the Graph 500 benchmark, but to our knowledge no one has attempted to run the Graph 500 benchmark on a public cloud yet.