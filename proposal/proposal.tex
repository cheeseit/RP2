\documentclass[A4]{scrartcl}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{mathtools}
%\usepackage[ddmmyyyyy]{datetime}

\usepackage[%
  backend=bibtex      % biber or bibtex
%,style=authoryear    % Alphabeticalsch
 ,style=numeric-comp  % numerical-compressed
 ,sorting=none        % no sorting
 ,sortcites=true      % some other example options ...
 ,block=none
 ,indexing=false
 ,citereset=none
 ,isbn=true
 ,url=true
 ,doi=true            % prints doi
 ,natbib=true         % if you need natbib functions
]{biblatex}
\bibliography{bib}



\begin{document}
\title{Graph500 in the public cloud.}
\subtitle{Research Project 2 - Proposal}
\date{\today}
\author{Harm Dermois \\ harm.dermois@os3.nl}

\maketitle

\section*{Introduction}
\label{sec:introduction}
Modern HPC applications are optimized to process 3D physics simulation, but are not well suited for analyzing graphs. The Graph500 \cite{murphy2010introducing} is an initiative to improve the state of algorithms, hardware architectures and software systems to better suit graph analysis/processing. Graph500 does this by having a top 500 list where different institutes can compete and find out who has the best graph processor.

To get on the list, a benchmark should be run. This benchmark needs to comply to some specifications which can be found on this website\cite{graph500-specs}, but beyond these specifications anything goes. This means that the benchmark can be optimized for the hardware it runs on. 

The benchmark consist of two kernels, the first being the graph construction, the second is the Breadth-First Search. Both of these kernels will be timed. The kernels are preceded by the creation of an edge-list using a Kronecker Generator\cite{leskovec2010kronecker}. The results of both kernels are validated and the performance information is output. 

The list, at the moment, consists mostly of super computers. The aim of this project try and put an entry on the list using the public cloud. The focus will be on finding a model to make a prediction of how many machines will be needed to get a certain performance.

\section*{Research Question}
During this project the following questions will be answered:
\begin{itemize}
\item How well does the reference implementation\cite{graph500-code} run on the public cloud?
\item How can this implementation be improved to get a better performance while running it in the cloud?
\item What size of graph is reasonable to benchmark while working in the cloud?
\item Is it possible to make a model that can predict the performance depending on the amount of resources?
\end{itemize}

\section*{Related Work}
There is a lot of related work on the Graph500 benchmark and implementation of the breadth-first search algorithms. 

This paper, by Chaktranont et al.\cite{chakthranont2014exploring}, shows the difference between Graph500 benchmark on a virtual private cluster and on a physical cluster. The MPI implementation variant of the benchmark were used for these benchmarks.

A paper by Beamer et al.\cite{beamer2011searching} shows an improved algorithm for doing a breadth-first search. This algorithm is a hybrid, combining the top down approach with the bottom up approach.

Koji Ueno and Toyotaro Suzumura\cite{ueno2012highly} investigated the reference Graph500 implementation. They point out the flaws of the MPI-based implementations which all do 1D partitioning and propose a 2D solution. The paper shows the results of this implementation which performs better for larger scale problems.

\section*{Scope}
The research will focus on running the MPI Graph500 reference implementation. The other benchmarks will not be considered. Optimization will only be done for running the implementation on the cloud.


\section*{Methodology}
The project will be consist of the following steps:
\begin{itemize}
\item Run the reference implementation locally.
\item Run the reference implementation on the DAS-4 from the Vrije Universiteit.
\item Run the reference implementation on the cloud.
\item Compare the results and possibly change the measurements.
\item Make a model from the results.
\item Optimize the reference implementation for running in the cloud.
\item Adjust the model.
\end{itemize}


\section*{Ethical Considerations}
There are no ethical considerations.

\printbibliography

\end{document}
